{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Korean MRC Baseline\n\n## Dependency\n다음과 같은 라이브러리를 사용한다.\n- [Konlpy](https://konlpy.org/ko/latest/index.html): 파이썬 한국어 NLP 처리기\n- [Mecab-korean](https://bitbucket.org/eunjeon/mecab-ko-dic/src): 한국어 형태소 분석기","metadata":{}},{"cell_type":"code","source":"! apt-get install -y openjdk-8-jdk python3-dev\n! pip install konlpy \"tweepy<4.0.0\"\n! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T08:23:53.548281Z","iopub.execute_input":"2022-10-04T08:23:53.54891Z","iopub.status.idle":"2022-10-04T08:24:06.698515Z","shell.execute_reply.started":"2022-10-04T08:23:53.548827Z","shell.execute_reply":"2022-10-04T08:24:06.697324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터셋 구성\n현재 JSON 데이터를 볼 수 있는 클래스를 하나 작성하자.","metadata":{}},{"cell_type":"code","source":"from typing import List, Tuple, Dict, Any\nimport json\nimport random\n\nclass KoMRC:\n    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n        self._data = data\n        self._indices = indices\n\n    # Json을 불러오는 메소드\n    @classmethod\n    def load(cls, file_path: str):\n        with open(file_path, 'r', encoding='utf-8') as fd:\n            data = json.load(fd)\n\n        indices = []\n        for d_id, document in enumerate(data['data']):\n            for p_id, paragraph in enumerate(document['paragraphs']):\n                for q_id, _ in enumerate(paragraph['qas']):\n                    indices.append((d_id, p_id, q_id))\n        \n        return cls(data, indices)\n\n    # 데이터 셋을 잘라내는 메소드\n    @classmethod\n    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n        indices = list(dataset._indices)\n        random.seed(seed)\n        random.shuffle(indices)\n        train_indices = indices[int(len(indices) * eval_ratio):]\n        eval_indices = indices[:int(len(indices) * eval_ratio)]\n\n        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n\n    def __getitem__(self, index: int) -> Dict[str, Any]:\n        d_id, p_id, q_id = self._indices[index]\n        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n\n        context = paragraph['context']\n        qa = paragraph['qas'][q_id]\n\n        guid = qa['guid']\n        question = qa['question']\n        answers = qa['answers']\n\n        return {\n            'guid': guid,\n            'context': context,\n            'question': question,\n            'answers': answers\n        }\n\n    def __len__(self) -> int:\n        return len(self._indices)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`load` 메소드를 이용해서 Json 데이터를 불러올 수 있다.","metadata":{}},{"cell_type":"code","source":"\ndataset = KoMRC.load('/kaggle/input/k-digital-goorm-5-korean-mrc/train.json')\nprint(\"Number of Samples:\", len(dataset))\nprint(dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`split` 메소드를 이용하면 데이터 셋을 나눌 수 있다.","metadata":{}},{"cell_type":"code","source":"train_dataset, dev_dataset = KoMRC.split(dataset)\nprint(\"Number of Train Samples:\", len(train_dataset))\nprint(\"Number of Dev Samples:\", len(dev_dataset))\nprint(dev_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"단어 단위로 토큰화해서 정답 위치를 찾기 위하여 토큰화 및 위치 인덱싱을 하는 클래스를 상속을 통해 작성해 보자.","metadata":{}},{"cell_type":"code","source":"from typing import Generator\n\nimport konlpy\n\nclass TokenizedKoMRC(KoMRC):\n    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n        super().__init__(data, indices)\n        self._tagger = konlpy.tag.Mecab()\n\n    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n        position = 0\n        tokens = []\n        for morph in self._tagger.morphs(sentence):\n            position = sentence.find(morph, position)\n            tokens.append((morph, (position, position + len(morph))))\n            position += len(morph)\n        return tokens\n            \n    def __getitem__(self, index: int) -> Dict[str, Any]:\n        sample = super().__getitem__(index)\n\n        context, position = zip(*self._tokenize_with_position(sample['context']))\n        context, position = list(context), list(position)\n        question = self._tagger.morphs(sample['question'])\n\n        if sample['answers'] is not None:\n            answers = []\n            for answer in sample['answers']:\n                for start, (position_start, position_end) in enumerate(position):\n                    if position_start <= answer['answer_start'] < position_end:\n                        break\n                else:\n                    print(context, answer)\n                    raise ValueError(\"No mathced start position\")\n\n                target = ''.join(answer['text'].split(' '))\n                source = ''\n                for end, morph in enumerate(context[start:], start):\n                    source += morph\n                    if target in source:\n                        break\n                else:\n                    print(context, answer)\n                    raise ValueError(\"No Matched end position\")\n\n                answers.append({\n                    'start': start,\n                    'end': end\n                })\n        else:\n            answers = None\n        \n        return {\n            'guid': sample['guid'],\n            'context_original': sample['context'],\n            'context_position': position,\n            'question_original': sample['question'],\n            'context': context,\n            'question': question,\n            'answers': answers\n        }\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-5-korean-mrc/train.json')\n\ntrain_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\nprint(\"Number of Train Samples:\", len(train_dataset))\nprint(\"Number of Dev Samples:\", len(dev_dataset))\nprint(dev_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = dev_dataset[0]\nprint(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vocab 생성 및 Indexing\n토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 `Indexer`를 만들자.","metadata":{}},{"cell_type":"code","source":"from typing import Sequence\nfrom collections import Counter\nfrom itertools import chain\n\nfrom tqdm.notebook import tqdm\n\nclass Indexer:\n    def __init__(self,\n        id2token: List[str], \n        max_length: int=1024,\n        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'\n    ):\n        self.pad = pad\n        self.unk = unk\n        self.cls = cls\n        self.sep = sep\n        self.special_tokens = [pad, unk, cls, sep]\n\n        self.max_length = max_length\n\n        self.id2token = self.special_tokens + id2token\n        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n\n    @property\n    def vocab_size(self):\n        return len(self.id2token)\n    \n    @property\n    def pad_id(self):\n        return self.token2id[self.pad]\n    @property\n    def unk_id(self):\n        return self.token2id[self.unk]\n    @property\n    def cls_id(self):\n        return self.token2id[self.cls]\n    @property\n    def sep_id(self):\n        return self.token2id[self.sep]\n\n    @classmethod\n    def build_vocab(cls,\n        dataset: TokenizedKoMRC, \n        min_freq: int=5\n    ):\n        counter = Counter(chain.from_iterable(\n            sample['context'] + sample['question']\n            for sample in tqdm(dataset, desc=\"Counting Vocab\")\n        ))\n\n        return cls([word for word, count in counter.items() if count >= min_freq])\n    \n    def decode(self,\n        token_ids: Sequence[int]\n    ):\n        return [self.id2token[token_id] for token_id in token_ids]\n\n    def sample2ids(self,\n        sample: Dict[str, Any],\n    ) -> Dict[str, Any]:\n        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]\n        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]\n\n        context = context[:self.max_length-len(question)-3]             # Truncate context\n        \n        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n\n        if sample['answers'] is not None:\n            answer = sample['answers'][0]\n            start = min(answer['start'] + len(question) + 2, self.max_length - 1)\n            end = min(answer['end'] + len(question) + 2, self.max_length - 1)\n        else:\n            start = None\n            end = None\n\n        return {\n            'guid': sample['guid'],\n            'context': sample['context_original'],\n            'question': sample['question_original'],\n            'position': sample['context_position'],\n            'input_ids': input_ids,\n            'token_type_ids': token_type_ids,\n            'start': start,\n            'end': end\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexer = Indexer.build_vocab(dataset)\nprint(indexer.sample2ids(dev_dataset[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"쉽게 Indexer를 활용하기 위해 Indexer가 포함된 데이터 셋을 만들자.","metadata":{}},{"cell_type":"code","source":"class IndexerWrappedDataset:\n    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n        self._dataset = dataset\n        self._indexer = indexer\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n    \n    def __getitem__(self, index: int) -> Dict[str, Any]:\n        sample = self._indexer.sample2ids(self._dataset[index])\n        sample['attention_mask'] = [1] * len(sample['input_ids'])\n\n        return sample\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\nindexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n\nsample = indexed_dev_dataset[0]\nprint(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Encoder를 활용한 MRC 모델\n![Bert for MRC](https://miro.medium.com/max/340/1*cXDOP0gsE7Zp8-sgZqYfTA.png)\n\nTransformer 인코더 마지막에 Linear Layer를 붙여 정답의 시작과 끝을 맞추는 간단한 모델을 생성보자.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nfrom transformers.models.bert.modeling_bert import (\n    BertModel,\n    BertPreTrainedModel\n)\n\n## Simple Version for Bert QA: https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForQuestionAnswering.forward\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.start_linear = nn.Linear(config.hidden_size, 1)\n        self.end_linear = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None\n    ):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n\n        start_logits = self.start_linear(outputs.last_hidden_state).squeeze(-1)\n        end_logits = self.end_linear(outputs.last_hidden_state).squeeze(-1)\n\n        return start_logits, end_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 학습 준비","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Collator:\n    def __init__(self, indexer: Indexer) -> None:\n        self._indexer = indexer\n\n    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        samples = {\n            key: [sample[key] for sample in samples]\n            for key in samples[0]\n        }\n\n        for key in 'start', 'end':\n            if samples[key][0] is None:\n                samples[key] = None\n            else:\n                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n            samples[key] = pad_sequence(\n                [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n                batch_first=True, padding_value=self._indexer.pad_id\n            )\n\n        return samples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 64\naccumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n\ncollator = Collator(indexer)\ntrain_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\ndev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(dev_loader))\nprint(batch['input_ids'].shape)\nprint(batch['input_ids'])\nprint(list(batch.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertConfig\n\ntorch.manual_seed(42)\nconfig = BertConfig(\n     vocab_size=indexer.vocab_size,\n     max_position_embeddings=1024,\n     hidden_size=256,\n     num_hidden_layers=4,\n     num_attention_heads=4,\n     intermediate_size=1024\n)\nmodel = BertForQuestionAnswering(config)\n# model.cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom statistics import mean\n\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\n\nos.makedirs('dump', exist_ok=True)\ntrain_losses = []\ndev_losses = []\n\nstep = 0\n\nfor epoch in range(1, 31):\n    print(\"Epoch\", epoch)\n    # Training\n    running_loss = 0.\n    losses = []\n    progress_bar = tqdm(train_loader, desc='Train')\n    for batch in progress_bar:\n        del batch['guid'], batch['context'], batch['question'], batch['position']\n        # batch = {key: value.cuda() for key, value in batch.items()}\n        start = batch.pop('start')\n        end = batch.pop('end')\n        \n        start_logits, end_logits = model(**batch)\n        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n        (loss / accumulation).backward()\n        running_loss += loss.item()\n        del batch, start, end, start_logits, end_logits, loss\n        \n        step += 1\n        if step % accumulation:\n            continue\n\n        clip_grad_norm_(model.parameters(), max_norm=1.)\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n\n        losses.append(running_loss / accumulation)\n        running_loss = 0.\n        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n    train_losses.append(mean(losses))\n    print(f\"train score: {train_losses[-1]:.3f}\")\n\n    # Evaluation\n    losses = []\n    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n        del batch['guid'], batch['context'], batch['question'], batch['position']\n        # batch = {key: value.cuda() for key, value in batch.items()}\n        start = batch.pop('start')\n        end = batch.pop('end')\n        \n        with torch.no_grad():\n            start_logits, end_logits = model(**batch)\n        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n\n        losses.append(loss.item())\n        del batch, start, end, start_logits, end_logits, loss\n    dev_losses.append(mean(losses))\n    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n\n    model.save_pretrained(f'dump/model.{epoch}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nt = list(range(1, 31))\nplt.plot(t, train_losses, label=\"Train Loss\")\nplt.plot(t, dev_losses, label=\"Dev Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![loss_plot](https://github.com/mynsng/mynsng.github.io/blob/master/assets/images/__results___26_0.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"학습 데이터 셋에 Overfitting이 일어나는 것을 확인할 수 있다.","metadata":{}},{"cell_type":"markdown","source":"## Answer Inference\n모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자.","metadata":{}},{"cell_type":"code","source":"model = BertForQuestionAnswering.from_pretrained('dump/model.30')\nmodel.cuda()\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, sample in zip(range(1, 4), indexed_train_dataset):\n    print(f'------{idx}------')\n    print('Context:', sample['context'])\n    print('Question:', sample['question'])\n    \n    input_ids, token_type_ids = [\n        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n        for key in (\"input_ids\", \"token_type_ids\")\n    ]\n    \n    with torch.no_grad():\n        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n    start_logits.squeeze_(0), end_logits.squeeze_(0)\n    \n    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n    index = torch.argmax(probability).item()\n    \n    start = index // len(end_prob)\n    end = index % len(end_prob)\n    \n    start = sample['position'][start][0]\n    end = sample['position'][end][1]\n\n    print('Answer:', sample['context'][start:end])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test 출력 파일 작성","metadata":{}},{"cell_type":"code","source":"test_dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-5-korean-mrc/test.json')\ntest_dataset = IndexerWrappedDataset(test_dataset, indexer)\nprint(\"Number of Test Samples\", len(test_dataset))\nprint(test_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nos.makedirs('out', exist_ok=True)\nwith torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n    writer = csv.writer(fd)\n    writer.writerow(['Id', 'Predicted'])\n\n    rows = []\n    for sample in tqdm(test_dataset, \"Testing\"):\n        input_ids, token_type_ids = [\n            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n            for key in (\"input_ids\", \"token_type_ids\")\n        ]\n    \n        with torch.no_grad():\n            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n        start_logits.squeeze_(0), end_logits.squeeze_(0)\n    \n        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n        index = torch.argmax(probability).item()\n    \n        start = index // len(end_prob)\n        end = index % len(end_prob)\n    \n        start = sample['position'][start][0]\n        end = sample['position'][end][1]\n\n        rows.append([sample[\"guid\"], sample['context'][start:end]])\n    \n    writer.writerows(rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}